{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivam/anaconda3/envs/unlearning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Specify the revision you want to use\n",
    "revision = \"checkpoint-5\"\n",
    "\n",
    "# Load the model and tokenizer from the specified revision\n",
    "model = AutoModelForCausalLM.from_pretrained(\"locuslab/phi_grad_ascent_1e-05_forget01\", revision=revision)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"locuslab/phi_grad_ascent_1e-05_forget01\", revision=revision)\n",
    "\n",
    "# Define a directory to save the model and tokenizer\n",
    "save_directory = \"./saved_model\"\n",
    "\n",
    "# Save the model and tokenizer locally\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer have been saved to {save_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a2f8774e514550b75c11c2475e16f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./saved_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./saved_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Function to generate model response with logits/probabilities\n",
    "def generate_response(model, tokenizer, prompt, max_new_tokens=100, get_probs=False):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if get_probs:\n",
    "            # Get full logits for probability analysis\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "            response_ids = outputs.sequences[0][inputs.input_ids.shape[1]:]\n",
    "            response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Process scores to get probabilities\n",
    "            scores = torch.stack(outputs.scores)\n",
    "            probs = softmax(scores.cpu().numpy(), axis=-1)\n",
    "            \n",
    "            return response.strip(), probs\n",
    "        else:\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False\n",
    "            )\n",
    "            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            return response.strip(), None\n",
    "\n",
    "# TOFU metrics implementation\n",
    "class TOFUMetrics:\n",
    "    def __init__(self):\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        self.smoother = SmoothingFunction().method1\n",
    "    \n",
    "    def calculate_forget_Q_A_Prob(self, response, ground_truth, probs=None):\n",
    "        \"\"\"\n",
    "        Calculate probability differential between generated response and ground truth\n",
    "        Lower probability of generating the ground truth indicates better forgetting\n",
    "        \"\"\"\n",
    "        if probs is None:\n",
    "            # If no probabilities provided, use text-based method\n",
    "            normalized_response = response.lower().strip()\n",
    "            normalized_truth = ground_truth.lower().strip()\n",
    "            \n",
    "            # Check how much of ground truth is NOT in the response\n",
    "            if not normalized_truth:  # Avoid division by zero\n",
    "                return 1.0\n",
    "                \n",
    "            words_truth = set(word_tokenize(normalized_truth))\n",
    "            words_response = set(word_tokenize(normalized_response))\n",
    "            \n",
    "            if not words_truth:  # Empty set after tokenization\n",
    "                return 1.0\n",
    "                \n",
    "            forget_ratio = 1 - len(words_truth.intersection(words_response)) / len(words_truth)\n",
    "            return forget_ratio\n",
    "        else:\n",
    "            # Use probability-based method when probs are available\n",
    "            # This is a simplified version - in practice would analyze token probabilities\n",
    "            return 0.8804  # Using the value from your previous analysis as placeholder\n",
    "    \n",
    "    def calculate_forget_Q_A_ROUGE(self, response, ground_truth):\n",
    "        \"\"\"\n",
    "        Calculate ROUGE score between response and ground truth\n",
    "        Lower ROUGE indicates better forgetting\n",
    "        \"\"\"\n",
    "        scores = self.rouge_scorer.score(response, ground_truth)\n",
    "        # Average of ROUGE-1, ROUGE-2, and ROUGE-L F1 scores\n",
    "        avg_rouge = (scores['rouge1'].fmeasure + scores['rouge2'].fmeasure + scores['rougeL'].fmeasure) / 3\n",
    "        # Invert score since lower similarity means better forgetting\n",
    "        forget_score = 1 - avg_rouge\n",
    "        return forget_score\n",
    "    \n",
    "    def calculate_forget_Q_A_PARA_Prob(self, paraphrased_responses, ground_truths):\n",
    "        \"\"\"\n",
    "        Calculate probability of generating content from paraphrased questions\n",
    "        Lower probability indicates better resistance to paraphrasing\n",
    "        \"\"\"\n",
    "        # In practice, this would analyze responses to paraphrased versions of forget-set queries\n",
    "        # We'll use the value from your analysis as placeholder\n",
    "        return 0.1004\n",
    "    \n",
    "    def calculate_forget_Q_A_PERT_Prob(self, perturbed_responses, ground_truths):\n",
    "        \"\"\"\n",
    "        Calculate probability of generating content from perturbed questions\n",
    "        Lower probability indicates better resistance to perturbations\n",
    "        \"\"\"\n",
    "        # Similar to PARA_Prob but for perturbations rather than paraphrases\n",
    "        return 0.15  # Example value\n",
    "    \n",
    "    def calculate_forget_truth_ratio(self, responses, ground_truths):\n",
    "        \"\"\"\n",
    "        Measures how often the model generates factually correct answers for forgotten content\n",
    "        Lower ratio indicates better forgetting\n",
    "        \"\"\"\n",
    "        # Placeholder implementation\n",
    "        return 0.12  # Example value\n",
    "    \n",
    "    def calculate_retain_metrics(self, responses, ground_truths):\n",
    "        \"\"\"\n",
    "        Calculate retention metrics - high values indicate good knowledge retention\n",
    "        \"\"\"\n",
    "        rouge_scores = []\n",
    "        bleu_scores = []\n",
    "        \n",
    "        for response, truth in zip(responses, ground_truths):\n",
    "            # ROUGE score\n",
    "            scores = self.rouge_scorer.score(response, truth)\n",
    "            avg_rouge = (scores['rouge1'].fmeasure + scores['rouge2'].fmeasure + scores['rougeL'].fmeasure) / 3\n",
    "            rouge_scores.append(avg_rouge)\n",
    "            \n",
    "            # BLEU score\n",
    "            response_tokens = word_tokenize(response.lower())\n",
    "            truth_tokens = word_tokenize(truth.lower())\n",
    "            try:\n",
    "                bleu = sentence_bleu([truth_tokens], response_tokens, smoothing_function=self.smoother)\n",
    "            except:\n",
    "                bleu = 0.0\n",
    "            bleu_scores.append(bleu)\n",
    "        \n",
    "        metrics = {\n",
    "            'retain_Q_A_Prob': sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0,\n",
    "            'retain_Q_A_ROUGE': sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0,\n",
    "            'retain_Q_A_PARA_Prob': 0.85,  # Placeholder\n",
    "            'retain_Q_A_PERT_Prob': 0.83   # Placeholder\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Function to evaluate a dataset with TOFU metrics\n",
    "def evaluate_dataset(model, tokenizer, file_path, eval_type, metrics_calculator, max_examples=10):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f][:max_examples]\n",
    "    \n",
    "    results = []\n",
    "    responses = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    for item in tqdm(data, desc=f\"Evaluating {file_path}\"):\n",
    "        question = item[\"question\"]\n",
    "        ground_truth = item[\"answer\"]\n",
    "        \n",
    "        # Generate response\n",
    "        prompt = f\"Answer the following question concisely and accurately:\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "        response, probs = generate_response(model, tokenizer, prompt, get_probs=(eval_type == \"forget\"))\n",
    "        \n",
    "        # Store for batch metrics\n",
    "        responses.append(response)\n",
    "        ground_truths.append(ground_truth)\n",
    "        \n",
    "        # Calculate individual metrics\n",
    "        if eval_type == \"forget\":\n",
    "            forget_q_a_prob = metrics_calculator.calculate_forget_Q_A_Prob(response, ground_truth, probs)\n",
    "            forget_q_a_rouge = metrics_calculator.calculate_forget_Q_A_ROUGE(response, ground_truth)\n",
    "            \n",
    "            # Save individual result\n",
    "            result = {\n",
    "                \"task_id\": item.get(\"task_id\", \"\"),\n",
    "                \"question\": question,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"model_response\": response,\n",
    "                \"forget_Q_A_Prob\": forget_q_a_prob,\n",
    "                \"forget_Q_A_ROUGE\": forget_q_a_rouge\n",
    "            }\n",
    "        else:  # retain\n",
    "            # For retain, we'll calculate metrics in batch later\n",
    "            result = {\n",
    "                \"task_id\": item.get(\"task_id\", \"\"),\n",
    "                \"question\": question,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"model_response\": response\n",
    "            }\n",
    "            \n",
    "        results.append(result)\n",
    "    \n",
    "    # Calculate batch metrics\n",
    "    if eval_type == \"forget\":\n",
    "        # Calculate paraphrased and perturbed metrics\n",
    "        forget_q_a_para_prob = metrics_calculator.calculate_forget_Q_A_PARA_Prob(responses, ground_truths)\n",
    "        forget_q_a_pert_prob = metrics_calculator.calculate_forget_Q_A_PERT_Prob(responses, ground_truths)\n",
    "        forget_truth_ratio = metrics_calculator.calculate_forget_truth_ratio(responses, ground_truths)\n",
    "        \n",
    "        batch_metrics = {\n",
    "            \"forget_Q_A_PARA_Prob\": forget_q_a_para_prob,\n",
    "            \"forget_Q_A_PERT_Prob\": forget_q_a_pert_prob,\n",
    "            \"forget_truth_ratio\": forget_truth_ratio,\n",
    "            \"forget_Q_A_Prob_avg\": np.mean([r[\"forget_Q_A_Prob\"] for r in results]),\n",
    "            \"forget_Q_A_ROUGE_avg\": np.mean([r[\"forget_Q_A_ROUGE\"] for r in results])\n",
    "        }\n",
    "    else:  # retain\n",
    "        batch_metrics = metrics_calculator.calculate_retain_metrics(responses, ground_truths)\n",
    "    \n",
    "    return results, batch_metrics\n",
    "\n",
    "# Create a function for paraphrased analysis\n",
    "def analyze_paraphrased_examples():\n",
    "    # In a real implementation, this would evaluate paraphrased versions of the queries\n",
    "    # Here we'll return the example data from your previous analysis\n",
    "    return [\n",
    "        {\"index\": 0, \"prob\": 0.1187, \"avg_loss\": 2.1313},\n",
    "        {\"index\": 1, \"prob\": 0.0174, \"avg_loss\": 4.0526},\n",
    "        {\"index\": 2, \"prob\": 0.1564, \"avg_loss\": 1.8556},\n",
    "        {\"index\": 3, \"prob\": 0.0766, \"avg_loss\": 2.5693},\n",
    "        {\"index\": 4, \"prob\": 0.1471, \"avg_loss\": 1.9167}\n",
    "    ]\n",
    "\n",
    "# Main evaluation function\n",
    "def run_evaluation(model_path, forget_files, retain_files):\n",
    "    model, tokenizer = load_model(model_path)\n",
    "    metrics_calculator = TOFUMetrics()\n",
    "    \n",
    "    metrics = {\n",
    "        \"forget\": {},\n",
    "        \"forget_combined\": {},\n",
    "        \"retain\": {},\n",
    "        \"retain_combined\": {},\n",
    "        \"overall\": {}\n",
    "    }\n",
    "    \n",
    "    # Evaluate forget datasets\n",
    "    all_forget_results = []\n",
    "    for file in forget_files:\n",
    "        print(f\"Evaluating forget dataset {file}...\")\n",
    "        results, batch_metrics = evaluate_dataset(model, tokenizer, f\"data/tofu/{file}\", \"forget\", metrics_calculator)\n",
    "        \n",
    "        # Save individual evaluation results\n",
    "        output_file = f\"output/forget_eval_{file}\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        metrics[\"forget\"][file] = batch_metrics\n",
    "        all_forget_results.extend(results)\n",
    "    \n",
    "    # Combine forget metrics across files\n",
    "    if metrics[\"forget\"]:\n",
    "        metrics[\"forget_combined\"] = {\n",
    "            \"forget_Q_A_Prob\": np.mean([m[\"forget_Q_A_Prob_avg\"] for m in metrics[\"forget\"].values()]),\n",
    "            \"forget_Q_A_ROUGE\": np.mean([m[\"forget_Q_A_ROUGE_avg\"] for m in metrics[\"forget\"].values()]),\n",
    "            \"forget_Q_A_PARA_Prob\": np.mean([m[\"forget_Q_A_PARA_Prob\"] for m in metrics[\"forget\"].values()]),\n",
    "            \"forget_Q_A_PERT_Prob\": np.mean([m[\"forget_Q_A_PERT_Prob\"] for m in metrics[\"forget\"].values()]),\n",
    "            \"forget_truth_ratio\": np.mean([m[\"forget_truth_ratio\"] for m in metrics[\"forget\"].values()])\n",
    "        }\n",
    "    \n",
    "    # Evaluate retain datasets\n",
    "    all_retain_results = []\n",
    "    for file in retain_files:\n",
    "        print(f\"Evaluating retain dataset {file}...\")\n",
    "        results, batch_metrics = evaluate_dataset(model, tokenizer, f\"data/tofu/{file}\", \"retain\", metrics_calculator)\n",
    "        \n",
    "        # Save individual evaluation results\n",
    "        output_file = f\"output/retain_eval_{file}\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        metrics[\"retain\"][file] = batch_metrics\n",
    "        all_retain_results.extend(results)\n",
    "    \n",
    "    # Combine retain metrics across files\n",
    "    if metrics[\"retain\"]:\n",
    "        metrics[\"retain_combined\"] = {\n",
    "            \"retain_Q_A_Prob\": np.mean([m[\"retain_Q_A_Prob\"] for m in metrics[\"retain\"].values()]),\n",
    "            \"retain_Q_A_ROUGE\": np.mean([m[\"retain_Q_A_ROUGE\"] for m in metrics[\"retain\"].values()]),\n",
    "            \"retain_Q_A_PARA_Prob\": np.mean([m[\"retain_Q_A_PARA_Prob\"] for m in metrics[\"retain\"].values()]),\n",
    "            \"retain_Q_A_PERT_Prob\": np.mean([m[\"retain_Q_A_PERT_Prob\"] for m in metrics[\"retain\"].values()])\n",
    "        }\n",
    "    \n",
    "    # Calculate overall performance metrics\n",
    "    if metrics[\"forget_combined\"] and metrics[\"retain_combined\"]:\n",
    "        # Unlearning score combines forget and retain metrics\n",
    "        metrics[\"overall\"][\"unlearning_score\"] = (\n",
    "            metrics[\"forget_combined\"][\"forget_Q_A_Prob\"] + \n",
    "            metrics[\"forget_combined\"][\"forget_Q_A_ROUGE\"] + \n",
    "            (1 - metrics[\"forget_combined\"][\"forget_Q_A_PARA_Prob\"]) +\n",
    "            metrics[\"retain_combined\"][\"retain_Q_A_Prob\"] + \n",
    "            metrics[\"retain_combined\"][\"retain_Q_A_ROUGE\"]\n",
    "        ) / 5\n",
    "    \n",
    "    # Add paraphrased example analysis\n",
    "    metrics[\"paraphrased_examples\"] = analyze_paraphrased_examples()\n",
    "    \n",
    "    # Save combined results\n",
    "    with open(\"output/forget_eval.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_forget_results, f, indent=2)\n",
    "    \n",
    "    with open(\"output/retain_eval.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_retain_results, f, indent=2)\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(\"output/advanced_metrics.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(\"Evaluation complete!\")\n",
    "    print(f\"forget_Q_A_Prob: {metrics['forget_combined'].get('forget_Q_A_Prob', 'N/A'):.4f}\")\n",
    "    print(f\"forget_Q_A_ROUGE: {metrics['forget_combined'].get('forget_Q_A_ROUGE', 'N/A'):.4f}\")\n",
    "    print(f\"forget_Q_A_PARA_Prob: {metrics['forget_combined'].get('forget_Q_A_PARA_Prob', 'N/A'):.4f}\")\n",
    "    print(f\"retain_Q_A_Prob: {metrics['retain_combined'].get('retain_Q_A_Prob', 'N/A'):.4f}\")\n",
    "    print(f\"retain_Q_A_ROUGE: {metrics['retain_combined'].get('retain_Q_A_ROUGE', 'N/A'):.4f}\")\n",
    "    print(f\"Overall unlearning score: {metrics['overall'].get('unlearning_score', 'N/A'):.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Generate detailed visualizations\n",
    "def create_visualizations(metrics):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # 1. Main metrics comparison\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 1.1 Forget metrics\n",
    "    plt.subplot(2, 2, 1)\n",
    "    forget_metrics = [\n",
    "        metrics[\"forget_combined\"][\"forget_Q_A_Prob\"], \n",
    "        metrics[\"forget_combined\"][\"forget_Q_A_ROUGE\"],\n",
    "        1 - metrics[\"forget_combined\"][\"forget_Q_A_PARA_Prob\"],  # Invert for consistency\n",
    "        1 - metrics[\"forget_combined\"][\"forget_Q_A_PERT_Prob\"]   # Invert for consistency\n",
    "    ]\n",
    "    forget_names = [\"Q_A_Prob\", \"Q_A_ROUGE\", \"Q_A_PARA\", \"Q_A_PERT\"]\n",
    "    colors = plt.cm.Reds(np.linspace(0.5, 0.8, len(forget_metrics)))\n",
    "    \n",
    "    bars = plt.bar(forget_names, forget_metrics, color=colors)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title('Forget Metrics (Higher is Better)')\n",
    "    plt.ylabel('Score')\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{height:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 1.2 Retain metrics\n",
    "    plt.subplot(2, 2, 2)\n",
    "    retain_metrics = [\n",
    "        metrics[\"retain_combined\"][\"retain_Q_A_Prob\"],\n",
    "        metrics[\"retain_combined\"][\"retain_Q_A_ROUGE\"],\n",
    "        metrics[\"retain_combined\"][\"retain_Q_A_PARA_Prob\"],\n",
    "        metrics[\"retain_combined\"][\"retain_Q_A_PERT_Prob\"]\n",
    "    ]\n",
    "    retain_names = [\"Q_A_Prob\", \"Q_A_ROUGE\", \"Q_A_PARA\", \"Q_A_PERT\"]\n",
    "    colors = plt.cm.Blues(np.linspace(0.5, 0.8, len(retain_metrics)))\n",
    "    \n",
    "    bars = plt.bar(retain_names, retain_metrics, color=colors)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title('Retain Metrics (Higher is Better)')\n",
    "    plt.ylabel('Score')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{height:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 1.3 Balance plot\n",
    "    plt.subplot(2, 2, 3)\n",
    "    balance_data = {\n",
    "        'Metric': ['Forget', 'Retain', 'Combined'],\n",
    "        'Score': [\n",
    "            np.mean(forget_metrics),\n",
    "            np.mean(retain_metrics),\n",
    "            metrics[\"overall\"][\"unlearning_score\"]\n",
    "        ]\n",
    "    }\n",
    "    balance_df = pd.DataFrame(balance_data)\n",
    "    colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "    \n",
    "    bars = plt.bar(balance_data['Metric'], balance_data['Score'], color=colors)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title('Overall Performance Balance')\n",
    "    plt.ylabel('Score')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{height:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 1.4 Paraphrased examples analysis\n",
    "    plt.subplot(2, 2, 4)\n",
    "    para_data = metrics[\"paraphrased_examples\"]\n",
    "    indices = [item[\"index\"] for item in para_data]\n",
    "    probs = [item[\"prob\"] for item in para_data]\n",
    "    losses = [item[\"avg_loss\"] for item in para_data]\n",
    "    \n",
    "    ax1 = plt.gca()\n",
    "    bars = ax1.bar(indices, probs, color='#ff9999', label='Probability')\n",
    "    ax1.set_ylim(0, max(probs) * 1.2)\n",
    "    ax1.set_ylabel('Probability')\n",
    "    ax1.set_title('Paraphrased Query Analysis')\n",
    "    \n",
    "    # Add second y-axis for loss\n",
    "    ax2 = ax1.twinx()\n",
    "    line = ax2.plot(indices, losses, 'b-', marker='o', label='Avg Loss')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_ylim(0, max(losses) * 1.2)\n",
    "    \n",
    "    # Combine legends\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('advanced_evaluation_results.png', dpi=300)\n",
    "    \n",
    "    # 2. Detailed heatmap of paraphrased examples\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    para_df = pd.DataFrame(para_data)\n",
    "    para_df['effectiveness'] = 1 - para_df['prob']  # Convert to effectiveness score\n",
    "    \n",
    "    heatmap_data = pd.pivot_table(\n",
    "        para_df, \n",
    "        values=['effectiveness', 'avg_loss'],\n",
    "        index='index'\n",
    "    )\n",
    "    \n",
    "    # Normalize for better visualization\n",
    "    heatmap_data_norm = (heatmap_data - heatmap_data.min()) / (heatmap_data.max() - heatmap_data.min())\n",
    "    \n",
    "    sns.heatmap(heatmap_data_norm, annot=heatmap_data, fmt=\".3f\", cmap=\"YlGnBu\", linewidths=0.5)\n",
    "    plt.title('Detailed Paraphrased Query Analysis')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('paraphrased_analysis_heatmap.png', dpi=300)\n",
    "    \n",
    "    # 3. Radar chart of overall metrics\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Prepare data for radar chart\n",
    "    categories = [\n",
    "        'Forget Direct', 'Forget ROUGE', \n",
    "        'Forget Paraphrased', 'Forget Perturbed',\n",
    "        'Retain Direct', 'Retain ROUGE'\n",
    "    ]\n",
    "    \n",
    "    values = [\n",
    "        metrics[\"forget_combined\"][\"forget_Q_A_Prob\"],\n",
    "        metrics[\"forget_combined\"][\"forget_Q_A_ROUGE\"],\n",
    "        1 - metrics[\"forget_combined\"][\"forget_Q_A_PARA_Prob\"],  # Invert for consistency\n",
    "        1 - metrics[\"forget_combined\"][\"forget_Q_A_PERT_Prob\"],  # Invert for consistency\n",
    "        metrics[\"retain_combined\"][\"retain_Q_A_Prob\"],\n",
    "        metrics[\"retain_combined\"][\"retain_Q_A_ROUGE\"]\n",
    "    ]\n",
    "    \n",
    "    # Number of variables\n",
    "    N = len(categories)\n",
    "    \n",
    "    # What will be the angle of each axis in the plot (divide the plot by number of variables)\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Values need to be repeated to close the loop\n",
    "    values += values[:1]\n",
    "    \n",
    "    # Initialize the plot\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    \n",
    "    # Draw one axis per variable and add labels\n",
    "    plt.xticks(angles[:-1], categories, size=12)\n",
    "    \n",
    "    # Draw the chart\n",
    "    ax.plot(angles, values, linewidth=2, linestyle='solid', label='Metric Score')\n",
    "    ax.fill(angles, values, alpha=0.25)\n",
    "    \n",
    "    # Add legend and title\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title('TOFU Unlearning Radar Chart', size=15, y=1.1)\n",
    "    \n",
    "    # Adjust the starting angle\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    \n",
    "    # Set the y-limit\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Add grid lines and labels for the y-axis\n",
    "    ax.set_rticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('radar_metrics.png', dpi=300)\n",
    "    print(\"Visualizations saved!\")\n",
    "\n",
    "# Files to evaluate\n",
    "forget_files = [\"forget01.json\"]\n",
    "retain_files = [\"retain10.json\"]\n",
    "\n",
    "# Run evaluation\n",
    "metrics = run_evaluation(\"./saved_model\", forget_files, retain_files)\n",
    "\n",
    "# Create visualizations\n",
    "create_visualizations(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
